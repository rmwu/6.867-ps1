%\documentclass{amsart}

\documentclass{article}
\usepackage[a4paper,hmargin=15mm,vmargin=20mm]{geometry}
\usepackage[nosetup, colorlinks]{tony}
\usepackage{graphicx}

\usepackage{amsmath,amssymb}
\usepackage{mathpazo}
\usepackage{multicol}
\usepackage{diagbox}

%\usepackage{xcolor}
%\usepackage[printwatermark]{xwatermark}
%\newwatermark*[allpages,color=gray!50,angle=45,scale=3,xpos=0,ypos=0]{DRAFT}


\title{6.867: Problem Set 1}
\date{September 29, 2016}

\begin{document}
\maketitle

\begin{multicols}{2}
% % % % % % % % % %
%    PROBLEM 1
% % % % % % % % % %

\section{Gradient Descent}

\subsection{Batch Gradient Descent}

We implemented a general-purpose batch gradient descent procedure, as described in Bishop 5.2.4, with user-specified objective function and gradient thereof, initial guess, step size, and termination criterion.
We elected to terminate once the objective function change fell below a given threshold.
We tested our implementation against two functions: a sign-reversed multivariate Gaussian with mean~$\mu$ and covariance matrix~$\Sigma$
\begin{equation}
f(x; \mu, \Sigma) = -\frac{e^{-\frac12(x - \mu)^T \Sigma^{-1}(x - \mu)}}{\sqrt{(2\pi)^n \det\Sigma}}
\end{equation}
and a quadratic bowl
\begin{equation}
f(x; A, b) = \frac{1}{2}x^T Ax - x^T b,
\end{equation}
where $A$ is positive-definite.
For this exercise, our Gaussian parameters were $\mu = (10, 10)^T$, $\Sigma = 1000 \cdot I_2$.
For the quadratic bowl, we took
\[A = \left(\begin{array}{cc}10 & 5 \\5 & 10\end{array}\right)\]
and $b = (400, 400)^T$.
We present our results in Figure~\ref{fig:1.1}, where we illustrate the effect of the initial guess and step size for convergence on these two objective functions.

\begin{figure*}
   \centering
   \includegraphics[width=3in]{img/1-1-gauss.pdf}
   \includegraphics[width=3in]{img/1-1-quad.pdf}
   \includegraphics[width=3in]{img/1-1-etaGauss.pdf}
   \includegraphics[width=3in]{img/1-1-etaQuad.pdf}
   \caption{Effects of initial guess and step size on convergence. Bad initial guesses took much longer to converge (8428 and 5743 iterations) than good initial guesses (6292 and 372 iterations, respectively). The bottom two figures show the change in objective function over time. Larger step sizes (red and green) converged faster, if at all.}
   \label{fig:1.1}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=3in]{img/1-1-batch.pdf}
   \includegraphics[width=3in]{img/1-1-stoch.pdf}
   \includegraphics[width=3in]{img/1-1-batch-func.pdf}
   \includegraphics[width=3in]{img/1-1-stoch-func.pdf}
   \caption{Batch gradient descent (left) and stochastic gradient descent (right) on the sum of squares error for linear regression on 100 data points in $(\RR^{10},\RR)$. Observe that SGD requires more iterations to converge and fluctuates more, in both objective function and gradient, than batch gradient descent.}
   \label{fig:1.2}
\end{figure*}



\subsection{Numerical Gradient Approximation}
\label{subsec:grad-approx}

Recall that our gradient descent procedure requires the user to specify both the objective function \emph{and} its gradient. In our examples, these gradients had simple analytic forms, but in general objective functions might have very complicated gradients. Therefore, as a check on (or as a substitute for) the user-specified gradient, we implemented a gradient approximation routine.

We approximate the gradient of a general function~$f:\mathbb{R}^d\to\mathbb{R}$ at given point~$x^*$ by approximating its partial derivatives as the central difference
\begin{equation}
\label{eq:grad-approx}
\f{\pa f}{\pa x_i}(x^*) \approx \f{f(x^* + \f12\eps\hat x_i) - f(x^* - \f12 \eps\hat x_i)}{\eps},
\end{equation}
where $\eps > 0$ is some small user-supplied difference step and $\hat x_i$ is the unit vector in the $x_i$ direction. We compute this approximate partial derivative along each dimension of our domain and collect the terms into our numerical gradient approximation. 

Using our approximation procedure, we verified the analytic gradient calculations we made for the negative Gaussian and the quadratic bowl.
We obtained the best approximations when $\eps$ was on the order of the radius of curvature at the point of interest (indeed, tighter structure warrants a smaller $\eps$).
Too small an~$\eps$ resulted in rounding errors and other numerical instability, while too large an~$\eps$ overstretched the secant approximation our procedure relies on.

% this damn table disappears

%\begin{center}
%\begin{tabular}{|c|c|c|c|c|}\hline
%\label{tab:1.2}
%$\epsilon$ & 1000 & 100 & 10 & 1e-4 \\ \hline
%MSE & 2.0e-17 & 1.4e-18 & 3.8e-18 & 2.1e-12 \\
% \hline
%\end{tabular}
%\end{center}

\subsection{Stochastic Gradient Descent}

By design, batch gradient descent determines each update step by looking at the entire dataset and taking a gradient. This approach becomes unfeasible with large datasets, and does not extend easily to online settings. For these reasons, we instead use stochastic gradient descent (SGD), in which we approximate the gradient as follows.

Suppose our objective function takes the form
\begin{equation}
J(\theta) = \sum_i J_i(\theta)
\end{equation}
where each $J_i$ is presumably a term corresponding to a single data point. Whereas the batch gradient descent update step takes a gradient over all $J$:
\begin{equation}
\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t),
\end{equation}
the SGD update step has the form
\begin{equation} \theta_{t+1} = \theta_t - \eta_t \nabla J_i(\theta_t). \end{equation}

Note two key differences. We take $\nabla J_i$ as an approximation of $\nabla J$.
More interestingly, our step size is now time-dependent.
Because SGD is stochastic, it is unclear whether it converges.
But it can be shown that choosing a learning rate schedule $\eta_t$ satisfying the Robbins-Monro conditions ($\sum_{t=1}^\infty \eta_t$ diverges and $\sum_{t=1}^\infty \eta_t^2$ converges) guarantees convergence.

We implemented SGD with two stopping criteria: either gradient norm or change in objective function drop below a provided threshold.
In practice, we chose to set a threshold on change in objective function, since it is more readily interpretable.

We tested both batch and stochastic gradient descent on a ordinary least-squares fitting problem, where we minimized the sum of squares error.
We present the results of both tests in Figure~\ref{fig:1.2}.
Note that while batch gradient descent converges smoothly, the loss function fluctuates as SGD runs. In both cases, we eventually get convergence.


% % % % % % % % % %
%    PROBLEM 2
% % % % % % % % % %

\section{Linear Regression}

Before proceeding, recall that a linear basis function model for a dataset $(x^{(n)}, y^{(n)})$ given a fixed set of basis functions $\phi_i$ is a regression model that tries to find weights~$w_i$ for which we approximately have
\beq
y^{(n)} = w_0 + \sum_i w_i \phi_i(x^{(n)}).
\eeq
In other words, we transform the independent variable $x$ by basis functions~$\phi_i$ into some feature space, then perform linear regression with independent variables $\phi_i(x)$ and dependent variable~$y$.

We wrote a procedure for general linear basis function regression with user-specified data points and basis functions that found the maximum-likelihood weight vector analytically, via Bishop Equation 3.15, which we restate here:
\begin{equation}
\label{eq:max-likelihood-weights}
w_\tx{ml} = (\Phi^T\Phi)\inv\Phi^T Y
\end{equation}
where $Y$ is a vector of $y^{(n)}$ values from our dataset and $\Phi$ is the design matrix, given by
\begin{equation}
\Phi_{ij} = \phi_j(x^{(i)}).
\end{equation}

\subsection{Polynomial Basis Functions}

We tested our procedure by performing fits on a dataset of 11 points with polynomial basis functions up to some specified degree $M$.
The dataset was generated by applying noise to the values produced by the function
\begin{equation}
\label{eq:dataset-secret-func}
y(x) = \cos(\pi x) + \cos(2\pi x).
\end{equation}
We plot the results of these fits for varying values of degree~$M$ in Figure~\ref{fig:2.1-polybasis}.
Note the obvious underfitting and overfitting introduced by models with excessively low or high degrees.

\begin{figure*}
   \centering
   \includegraphics[width=3in]{img/2-1_degree0.pdf}
   \includegraphics[width=3in]{img/2-1_degree1.pdf}
   \includegraphics[width=3in]{img/2-1_degree3.pdf}
   \includegraphics[width=3in]{img/2-1_degree10.pdf}
   \caption{Plots of our polynomial basis fits for degrees $M=0,1,3,10$ (red) of our dataset (blue), which was generated from an underlying function (yellow).
   The degree ascends from top to bottom, and within each row, from left to right.}
   \label{fig:2.1-polybasis}
\end{figure*}

\subsection{Regression by Gradient Descent}

Though there is a closed-form formula for the best-fit weight vector (Equation~\ref{eq:max-likelihood-weights}), we decided to verify our results from the previous section through gradient descent.

To this end, we considered the sum-of-squares error (SSE) for our problem:
\begin{align*}
J(w) &= \f12 \sum_n \lt(y^{(n)} - \sum_j\phi_j(x^{(n)}) w_j\rt)^2 \\
&= \f12 (\Phi w - Y)^T(\Phi w - Y), \label{eq:sse}\numberthis
\end{align*}
which has gradient
\beq
\label{eq:sse-grad}
\nabla J(w) = \Phi^T(\Phi w - Y).
\eeq
We verified the correctness of Equation~\ref{eq:sse-grad} with our numeric gradient approximation from section~\ref{subsec:grad-approx}.

We proceeded to estimate the maximum-likelihood weight vector $w_\tx{ml}$ (Equation~\ref{eq:max-likelihood-weights}), which minimizes $J(w)$, using batch gradient descent on the SSE.
For small degree fits, results agreed strongly with the closed-form solution.
However, for higher degrees, discrepancies grew between our maximum-likelihood fits and our gradient-descent estimate fits (Figure~\ref{fig:bgd-poly-fits}).

\begin{figure*}[h]
   \centering
   \includegraphics[width=2.25in]{img/2-3_bgd_fit_degree1.pdf}
   \includegraphics[width=2.25in]{img/2-3_bgd_fit_degree4.pdf}
   \includegraphics[width=2.25in]{img/2-3_bgd_fit_degree7.pdf}
   \caption{Polynomial fits of various degrees by batch gradient descent (green) of the dataset (blue) generated from a sum of cosines (yellow).
   The fits agree perfectly with the maximum-likelihood fits (red) at low degrees, but differ increasingly for higher degrees.
   Weights were initialized to zero.
   }
   \label{fig:bgd-poly-fits}
\end{figure*}

Interestingly, the gradient descent fits did not overfit nearly as much as the maximum-likelihood fits at high degree.
This behavior likely arises because we initialize our weight vectors at 0, ``regularizing" our fits in a sense.
Indeed, initializing our weight vectors to very large (in absolute value) random numbers produced very high-variance fits.

Moreover, our gradient descent procedure terminates when the objective function changes between gradient descent iterations drops below a certain threshold, which we can think of as a form of early stopping.
Intuitively, overfitting requires a large change in our model parameters to get small decreases in our loss function; our convergence criterion prevents this sort of micro-optimization from occurring.

Varying the step size and the convergence threshold had little effect at any degree, unless we changed the parameters by many orders of magnitude. (e.g. large step sizes can lead to divergence)

We repeated this analysis with stochastic gradient descent (SGD), which was much more sensitive to our initial guess.
Unless we initialized our weights closed to the correct (maximum-likelihood) value, SGD produced very poor fits, even at low degree.
At higher degrees, even picking initial weights close to their correct value did not seem to help with convergence.

This behavior can be explained if we consider the size of our dataset.
With a small dataset, it is more likely that the algorithm will believe it has converged when it hasn't, since it is conceivable that we terminate after checking a point that is already fit well and noticing that the resulting weight update changes the objective function very little.


\subsection{Cosine Basis Functions}
\label{subsec:cosine-basis}

We also experimented with using cosine basis functions $\phi_m(x) = \cos(m\pi x)$ to fit the dataset.
Recall that the values were generated from Equation~\ref{eq:dataset-secret-func}, so we expect our weights to be $(0, 1, 1, 0, \dots)$ (recall that the first term is the weight for $\phi_0(x) \equiv 1$, so it is a bias term).
With a fit degree of~8, however, we overfitted significantly, and our weight vector differed greatly from the actual weights used to generate the data.
In section~\ref{sec:lasso} we will enforce sparsity on our linear models to produce better fits in certain scenarios.

% elaborate??


% % % % % % % % % %
%    PROBLEM 3
% % % % % % % % % %

\section{Ridge Regression}

Recall that ridge regression is just linear regression, with an $L^2$ regularization term added to the loss function. That is, our error is the following modification of Equation~\ref{eq:sse} (Bishop Equation~3.27):
\begin{equation}\label{eq:moderr}
J(w) = \f12 (\Phi w - Y)^T (\Phi w - Y) + \f12 \lambda |w_+|^2,
\end{equation}
where $w_+$ is the weight vector~$w$ \emph{without} the bias term and $\lambda\ge 0$ controls the strength of regularization. The extra term penalizes large weights.

Then it can be shown that the optimal weight vector is given by
\begin{equation}
\label{eq:ridge-weights}
w_{+,\tx{ridge}} = (\lambda I + Z^T Z)\inv Z^T Y_c
\end{equation}
where $Z=\Phi - \overline\Phi$, where $\Phi$ is taken with \emph{no} column of 1's, and where $\overline\Phi$ is the average of all rows of $\Phi$ (an average of data points); and where $Y_c$ is $Y - \bar Y$, where $\bar Y$ is the mean of $Y$; and bias term
\begin{equation}
\label{eq:ridge-bias}
w_{0, \tx{ridge}} = \overline Y - w_{+,\tx{ridge}}^T \overline \Phi
\end{equation}


We implemented ridge regression and tested it on the data from our previous experiments on regression for a variety of polynomial degrees and values of $\lambda$. We present some of the fits in Figure~\ref{fig:ridge}.

\begin{figure*}
   \centering
   \newlength{\picwidth}
   \setlength{\picwidth}{2in}
   \includegraphics[width=\picwidth]{img/3-1_ridge_lambd3_degree1.pdf}
   \includegraphics[width=\picwidth]{img/3-1_ridge_lambd3_degree3.pdf}
   \includegraphics[width=\picwidth]{img/3-1_ridge_lambd3_degree6.pdf}
   \includegraphics[width=\picwidth]{img/3-1_ridge_lambd30_degree1.pdf}
   \includegraphics[width=\picwidth]{img/3-1_ridge_lambd30_degree3.pdf}
   \includegraphics[width=\picwidth]{img/3-1_ridge_lambd30_degree6.pdf}
   \caption{Regularized (green) and non-regularized (red) polynomial fits for the sum-of-cosines dataset.}
   \label{fig:ridge}
\end{figure*}

Note that when fitting with higher order polynomial bases, even small values of $\lambda$ can drastically reduce overfitting. In general, at all degrees, larger values of $\lambda$ ``smooth" out and slightly ``flatten" our fit. Indeed, in the linear case, larger values of $\lambda$ would decrease the weight, i.e. the slope of our regression line.


\subsection{Model Selection}

We now turn our attention to another dataset, again of pairs of real numbers.
This dataset has been partitioned into sets $A$, $B$, and $V$, with respective sizes 13, 10, and 22, and is plotted in Figure~\ref{fig:3-2-bestfit}.
We will use $V$ as our validation set. We will try to model this data with a polynomial fit, as before, except we will be using regularization.

We used $A$ as training data and $B$ as test data, then vice versa.
In each case, we found the optimal weights by Equations~\ref{eq:ridge-weights} and \ref{eq:ridge-bias} on training data for many values of the degree $M$ of the polynomial basis and the regularization parameter $\lambda$. We chose the $(M^*, \lambda^*)$ that produced the lowest \emph{validation} error, then evaluated the model on test data.
Note that we evaluate performance of an actual weight vector with ordinary mean-square error (MSE; Equation~\ref{eq:sse} normalized by the number of data points), with \emph{no regularization term}.

We present the validation errors we obtained in Tables~\ref{tab:val-mse-a} and \ref{tab:val-mse-b}.
When we trained on $A$ and tested on $B$, the best hyperparameters were $(M^*,\lambda^*)=(2,0)$; the validation MSE was $0.05336$, and the test MSE was $1.28765$.
When we trained on $B$ and tested on $A$, the best hyperparameters $(M^*,\lambda^*)=(3,0.6)$ produced a validation MSE of $0.65195$ and a test MSE of $1.51697$.
We plot these two best-fit models in Figure~\ref{fig:3-2-bestfit}.


\begin{table*}
\caption{Mean-square error on our validation set after training on $A$. The optimal pair of hyperparameters was $(M^*,\lambda^*) = (2, 0)$.}
\centering
\begin{tabular}{|l||c|c|c|c|c|c|}
\hline
\backslashbox{$\lambda$}{$M$} & 0		& 1		 & 2	   & 3 & 4 & 5 \\\hline
0		& 1.40909 & 0.06584 & {\color{red}0.05336} & 0.05386 &  0.08134  & 0.09403\\
0.001	& " & 0.06584 & 0.05336 & 0.05388 &0.08129& 0.09393\\
0.003	& " & 0.06585 & 0.05337 & 0.05392 &0.08119& 0.09373\\
0.01	& " & 0.06587 & 0.05339 & 0.05407 &0.08086& 0.09305\\
0.03	& " & 0.06596 & 0.05345 & 0.05450 &0.07998& 0.09120\\
0.1		& " & 0.06624 & 0.05366 & 0.05600 &0.07747& 0.08564\\\hline
\end{tabular}
\label{tab:val-mse-a}
\end{table*}

\begin{table*}
\caption{Analogous to Table~\ref{tab:val-mse-a}, except with training on $B$. The best hyperparameters are $(M^*,\lambda^*)=(3,0.6)$.}
\centering
\begin{tabular}{|l||c|c|c|c|c|c|}
\hline
\backslashbox{$\lambda$}{$M$}&
0		& 1		 & 2	   & 3 & 4 & 5 \\\hline
0		& 1.64635 & 0.79978 & 0.87563 & 0.68864 &  2.95901  & 14.21443\\
%0.001	& " & 0.79981 & 0.87562 & 0.68843 & 2.95298 & 14.17389\\
%0.003	& " & 0.79987 & 0.87560 & 0.68801 & 2.94101 & 14.09364\\
%0.01	& " & 0.80008 & 0.87553 & 0.68656 & 2.89993 & 13.82113\\
0.03	& " & 0.80066 & 0.87533 & 0.68262 & 2.78933 & 13.10785\\
0.1		& " & 0.80271 & 0.87471 & 0.67110 & 2.46809 & 11.18677\\
0.3		& " & 0.80852 & 0.87347 & 0.65314 & 1.90417 & 8.20467\\
0.6		& " & 0.81709 & 0.87289 & \color{red}0.65195 & 1.51048 & 6.18385\\
1.0		& " & 0.82828 & 0.87410 & 0.67289 & 1.29261 & 4.82707
\\\hline
\end{tabular}
\label{tab:val-mse-b}
\end{table*}

\begin{figure*}
   \centering
   \includegraphics[width=2.5in]{img/3-2_lambda0_degree2.pdf}
   \includegraphics[width=2.5in]{img/3-2_lambda6_degree3.pdf}
   \caption{Best fit models for our dataset, where $A$ is plotted with blue dots, $B$ with green crosses, and $V$ with small plus signs. Note how the single outlier in $B$ skews the second fit significantly.}
   \label{fig:3-2-bestfit}
\end{figure*}

Before concluding, observe from Tables~\ref{tab:val-mse-a} and \ref{tab:val-mse-b} that increasing the degrees of freedom in our model increases the optimal $\lambda$. Indeed, more degrees of freedom require more regularization to prevent overfitting.


% % % % % % % % % %
%    PROBLEM 4
% % % % % % % % % %

\section{Sparsification with LASSO}
\label{sec:lasso}

In section~\ref{subsec:cosine-basis}, recall that we attempted a cosine basis fit of data generated by Equation~\ref{eq:dataset-secret-func}.
The model overfit the data significantly, as was apparent graphically, so we might therefore consider regularization.
Instead of ridge regression (quadratic regularization), however, we used the LASSO (least absolute shrinkage and selection operation), which enforces our prior beliefs about the sparsity of the weight vectors.

Recall that in ridge regression, we modified our loss function to include a quadratic regularizer. We can generalize Equation~\ref{eq:moderr} (Bishop 3.29) to other exponents. In particular, when the exponent is 1, we have the LASSO error function:
\begin{equation}J(w) = \f{1}{n}\sum_{i=1}^n{(y^{(i)}-w^T\phi(x^{(i)}))^2} + \lambda \sum_{j=1}^M{|w_j|}.\end{equation}

We evaluated LASSO against a dataset generated by a sparse linear combination of the 13 basis functions
\begin{equation}
\phi(x)=(x, \sin{(0.4\pi x\cdot1)},\dots,\sin({0.4\pi x\cdot 12})).
\end{equation}
In comparison with ridge regression and vanilla linear regression, weights produced by LASSO were sparse (as expected, since the underlying generator was sparse)---most coefficients $w_j$ were driven to 0 as $\lambda$ increased; such was not the case with ridge regression, which we also tested.
Consequently, LASSO fits generalized better, as is apparent from Figure~\ref{fig:lasso-weights}.

\begin{figure*}
   \centering
   \includegraphics[width=2.25in]{img/4-1-lasso-weights.pdf}
   \includegraphics[width=2.25in]{img/4-1-ridge-weights.pdf}
   \includegraphics[width=2.25in]{img/4-1-fits.pdf}
   \caption{The weights produced by LASSO (left) and ridge regression (center) as their regularization parameters vary. Note how LASSO drives most weights to zero, and how linear changes to $\lambda$ result in linear changes to the weights (the curvature is an artifact of our horizontal log axis). (Right) Various linear-model fits of the training data. Note how LASSO generalizes much better to the test and validation datasets.}
   \label{fig:lasso-weights}
\end{figure*}




\end{multicols}

\end{document}
